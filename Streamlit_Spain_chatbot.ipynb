{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q streamlit\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EoW19H6TTYjU",
        "outputId": "e8785672-7514-4bff-ef96-d5dd1cef80dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.4/23.4 MB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64\n",
        "!chmod +x cloudflared-linux-amd64\n",
        "!nohup /content/cloudflared-linux-amd64 tunnel --url http://localhost:8501 &"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_OGGFkaaT0XB",
        "outputId": "6939e3bf-e7aa-41ad-c5ee-ef50358580d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-13 15:55:08--  https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/cloudflare/cloudflared/releases/download/2024.12.1/cloudflared-linux-amd64 [following]\n",
            "--2024-12-13 15:55:09--  https://github.com/cloudflare/cloudflared/releases/download/2024.12.1/cloudflared-linux-amd64\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/106867604/d519add1-9e9f-4539-86ac-a9e32a76cf28?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20241213%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241213T155509Z&X-Amz-Expires=300&X-Amz-Signature=15ba0488459f9127be0a8db7f62310289922f880e7e7c65e97193dbac3826948&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dcloudflared-linux-amd64&response-content-type=application%2Foctet-stream [following]\n",
            "--2024-12-13 15:55:09--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/106867604/d519add1-9e9f-4539-86ac-a9e32a76cf28?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20241213%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241213T155509Z&X-Amz-Expires=300&X-Amz-Signature=15ba0488459f9127be0a8db7f62310289922f880e7e7c65e97193dbac3826948&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dcloudflared-linux-amd64&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 37526445 (36M) [application/octet-stream]\n",
            "Saving to: ‘cloudflared-linux-amd64’\n",
            "\n",
            "cloudflared-linux-a 100%[===================>]  35.79M   122MB/s    in 0.3s    \n",
            "\n",
            "2024-12-13 15:55:09 (122 MB/s) - ‘cloudflared-linux-amd64’ saved [37526445/37526445]\n",
            "\n",
            "nohup: appending output to 'nohup.out'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1gUVTLYg8hb",
        "outputId": "06034626-73dd-4e97-f9aa-719e04a16b6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 11.7 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 410.6/410.6 kB 21.8 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.5/2.5 MB 24.6 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 49.5/49.5 kB 2.9 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 27.5/27.5 MB 50.2 MB/s eta 0:00:00\n",
            "Processing file 1-D3XELIFrvYg1tvHCS0pixJ2r_O-xfFR index.faiss\n",
            "Processing file 1-Bxl5W5j69RqGPZO86hn9DjvZDmq0sPm index.pkl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retrieving folder contents\n",
            "Retrieving folder contents completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-D3XELIFrvYg1tvHCS0pixJ2r_O-xfFR\n",
            "To: /content/faiss_index_2/index.faiss\n",
            "\r  0%|          | 0.00/9.05M [00:00<?, ?B/s]\r100%|██████████| 9.05M/9.05M [00:00<00:00, 96.3MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-Bxl5W5j69RqGPZO86hn9DjvZDmq0sPm\n",
            "To: /content/faiss_index_2/index.pkl\n",
            "\r  0%|          | 0.00/5.01M [00:00<?, ?B/s]\r100%|██████████| 5.01M/5.01M [00:00<00:00, 242MB/s]\n",
            "Download completed\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "pip install -qqq -U langchain-huggingface\n",
        "pip install -qqq -U langchain\n",
        "pip install -qqq -U langchain-community\n",
        "pip install -qqq -U faiss-cpu\n",
        "\n",
        "\n",
        "# download saved vector database\n",
        "gdown --folder 1ej1-cIitZFitlXChtUoGfFONwL8yIsXA"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata # we stored our access token as a colab secret\n",
        "\n",
        "### Change this to your own token\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = userdata.get('UV_TOKEN')"
      ],
      "metadata": {
        "id": "FokjfEr-hUsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile spain_app.py\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_huggingface import HuggingFaceEndpoint, HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.chains import create_history_aware_retriever\n",
        "from langchain.chains.retrieval import create_retrieval_chain\n",
        "from langchain.chains.combine_documents.stuff import create_stuff_documents_chain\n",
        "import streamlit as st\n",
        "\n",
        "\n",
        "# This info's at the top of each HuggingFace model page\n",
        "hf_model = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "\n",
        "\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id = hf_model,\n",
        "    # max_new_tokens=512,\n",
        "    temperature=1,\n",
        "    top_p=0.99,\n",
        "    repetition_penalty=1.03\n",
        ")\n",
        "\n",
        "\n",
        "# embeddings\n",
        "embedding_model = \"sentence-transformers/all-MiniLM-l6-v2\"\n",
        "embeddings_folder = \"/content/\"\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(model_name=embedding_model,\n",
        "                                   cache_folder=embeddings_folder)\n",
        "\n",
        "\n",
        "# load Vector Database\n",
        "# allow_dangerous_deserialization is needed. Pickle files can be modified to deliver a malicious payload that results in execution of arbitrary code on your machine\n",
        "vector_db = FAISS.load_local(\"/content/faiss_index_2\", embeddings, allow_dangerous_deserialization=True)\n",
        "\n",
        "# retriever\n",
        "retriever = vector_db.as_retriever(search_kwargs={\"k\": 2})\n",
        "template = \"\"\"You are a nice chatbot having a conversation with a human. Answer the question based only on the following context and previous conversation. Keep your answers short and succinct.\n",
        "\n",
        "Previous conversation:\n",
        "{chat_history}\n",
        "\n",
        "Context to answer question:\n",
        "{context}\n",
        "\n",
        "New human question: {input}\n",
        "Response:\"\"\"\n",
        "\n",
        "#chat_history = []\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", template),\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    (\"human\", \"{input}\"),\n",
        "])\n",
        "\n",
        "\n",
        "# bot with memory\n",
        "@st.cache_resource\n",
        "def init_bot():\n",
        "    doc_retriever = create_history_aware_retriever(llm, retriever, prompt)\n",
        "    doc_chain = create_stuff_documents_chain(llm, prompt)\n",
        "    return create_retrieval_chain(doc_retriever, doc_chain)\n",
        "\n",
        "rag_bot = init_bot()\n",
        "\n",
        "\n",
        "##### streamlit #####\n",
        "\n",
        "st.title(\"Spain Tourist Guide\")\n",
        "\n",
        "# Initialise chat history\n",
        "# Chat history saves the previous messages to be displayed\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "# Display chat messages from history on app rerun\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.markdown(message[\"content\"])\n",
        "\n",
        "# React to user input\n",
        "if prompt := st.chat_input(\"Please enter your question\"):\n",
        "\n",
        "    # Display user message in chat message container\n",
        "    st.chat_message(\"human\").markdown(prompt)\n",
        "\n",
        "    # Add user message to chat history\n",
        "    st.session_state.messages.append({\"role\": \"human\", \"content\": prompt})\n",
        "\n",
        "    # Begin spinner before answering question so it's there for the duration\n",
        "    with st.spinner(\"Searching for an answer...\"):\n",
        "\n",
        "        # send question to chain to get answer\n",
        "        answer = rag_bot.invoke({\"input\": prompt, \"chat_history\": st.session_state.messages, \"context\": retriever})\n",
        "\n",
        "        # extract answer from dictionary returned by chain\n",
        "        response = answer[\"answer\"]\n",
        "\n",
        "        # Display chatbot response in chat message container\n",
        "        with st.chat_message(\"assistant\"):\n",
        "            st.markdown(response)\n",
        "\n",
        "        # Add assistant response to chat history\n",
        "        st.session_state.messages.append({\"role\": \"assistant\", \"content\":  response})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZAW9RIiWKpQ",
        "outputId": "7be0370f-7bd0-47c1-dc14-62701d85f33d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing spain_app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!grep -o 'https://.*\\.trycloudflare.com' nohup.out | head -n 1 | xargs -I {} echo \"Your tunnel url {}\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GV_FhQbgrfI",
        "outputId": "367adcb9-7d8e-4608-967d-3cffbe7273a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your tunnel url https://grill-myth-coupled-manuals.trycloudflare.com\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run spain_app.py &>/content/logs.txt &"
      ],
      "metadata": {
        "id": "eJ1W1Y8ygtXl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}